{"cells":[{"cell_type":"markdown","metadata":{},"source":["Mount drive"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1829,"status":"ok","timestamp":1717347891360,"user":{"displayName":"Carlos Vinagre","userId":"00694789698599923094"},"user_tz":-60},"id":"cR4cGdY82KFK","outputId":"1d308320-8783-4b64-b321-678453087157"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"markdown","metadata":{},"source":["1. Fine-tuning Imports"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tensorflow.keras.applications import VGG16"]},{"cell_type":"markdown","metadata":{},"source":["1. Fetching Image Directories"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import tensorflow as tf\n","\n","# Directories for training, validation, and test data\n","train_dir = '/content/drive/MyDrive/AI/Projeto_AI/train'\n","validation_dir = '/content/drive/MyDrive/AI/Projeto_AI/train5'\n","test_dir = '/content/drive/MyDrive/AI/Projeto_AI/test'\n"]},{"cell_type":"markdown","metadata":{"id":"G8YcakgN93d4"},"source":["2. Preprocessing Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define image size and batch size\n","IMG_SIZE = 150\n","BATCH_SIZE = 32\n","\n","# Load and preprocess data\n","train_dataset = tf.keras.utils.image_dataset_from_directory(\n","    train_dir,\n","    image_size=(IMG_SIZE, IMG_SIZE),\n","    batch_size=BATCH_SIZE\n",")\n","\n","validation_dataset = tf.keras.utils.image_dataset_from_directory(\n","    validation_dir,\n","    image_size=(IMG_SIZE, IMG_SIZE),\n","    batch_size=BATCH_SIZE\n",")\n","\n","test_dataset = tf.keras.utils.image_dataset_from_directory(\n","    test_dir,\n","    image_size=(IMG_SIZE, IMG_SIZE),\n","    batch_size=BATCH_SIZE\n",")\n"]},{"cell_type":"markdown","metadata":{},"source":["2. Preprocessing Data with Data Augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define image size and batch size\n","IMG_SIZE = 150\n","BATCH_SIZE = 32\n","\n","# Data augmentation\n","data_augmentation = tf.keras.Sequential([\n","    layers.RandomFlip(\"horizontal_and_vertical\"),\n","    layers.RandomRotation(0.2),\n","])\n","\n","# Load and preprocess data\n","train_dataset = tf.keras.utils.image_dataset_from_directory(\n","    train_dir,\n","    image_size=(IMG_SIZE, IMG_SIZE),\n","    batch_size=BATCH_SIZE\n",").map(lambda x, y: (data_augmentation(x, training=True), y))\n","\n","validation_dataset = tf.keras.utils.image_dataset_from_directory(\n","    validation_dir,\n","    image_size=(IMG_SIZE, IMG_SIZE),\n","    batch_size=BATCH_SIZE\n",")\n","\n","test_dataset = tf.keras.utils.image_dataset_from_directory(\n","    test_dir,\n","    image_size=(IMG_SIZE, IMG_SIZE),\n","    batch_size=BATCH_SIZE\n",")\n"]},{"cell_type":"markdown","metadata":{},"source":["3. Creation of Neural Model with Fine-Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Optimize data pipeline\n","AUTOTUNE = tf.data.AUTOTUNE\n","train_dataset = train_dataset.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n","validation_dataset = validation_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n","test_dataset = test_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n","\n","# 3. Creation of Neural Model (Feature Extraction)\n","base_model = VGG16(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n","\n","# Freeze the base model\n","base_model.trainable = False\n","\n","# Add custom classification layers on top\n","model = models.Sequential([\n","    data_augmentation,\n","    layers.Rescaling(1./255),\n","    base_model,\n","    layers.Flatten(),\n","    layers.Dense(512, activation='relu'),\n","    layers.Dropout(0.5),\n","    layers.Dense(len(train_dataset.class_names), activation='softmax')\n","])"]},{"cell_type":"markdown","metadata":{},"source":["4. Compilation of Neural Model with Fine-Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Unfreeze some layers of the base model\n","base_model.trainable = True\n","\n","# Let's take a look to see how many layers are in the base model\n","print(\"Number of layers in the base model: \", len(base_model.layers))\n","\n","# Fine-tune from this layer onwards\n","fine_tune_at = 10\n","\n","# Freeze all the layers before the `fine_tune_at` layer\n","for layer in base_model.layers[:fine_tune_at]:\n","    layer.trainable = False\n","\n","# Compile the model with a lower learning rate for fine-tuning\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{},"source":["5. Train the Model with Fine-Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Train the model (Fine-Tuning)\n","history_fine = model.fit(\n","    train_dataset,\n","    epochs=10,\n","    validation_data=validation_dataset\n",")\n","# Evaluate the model (Fine-Tuning)\n","test_loss, test_acc = model.evaluate(test_dataset)\n","print(f'Test accuracy (Fine-Tuning): {test_acc}')\n","\n","# Save the fine-tuned model\n","model.save('/content/drive/MyDrive/AI/Projeto_AI/image_classifier_model_fine_tuning.h5')"]},{"cell_type":"markdown","metadata":{"id":"1yQd8WrLPR9G"},"source":["3. Creation of Neural Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from keras import layers, models\n","\n","model = models.Sequential([\n","    layers.Rescaling(1./255, input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n","    layers.Conv2D(32, (3, 3), activation='relu'),\n","    layers.MaxPooling2D((2, 2)),\n","    layers.Conv2D(64, (3, 3), activation='relu'),\n","    layers.MaxPooling2D((2, 2)),\n","    layers.Conv2D(128, (3, 3), activation='relu'),\n","    layers.MaxPooling2D((2, 2)),\n","    layers.Conv2D(128, (3, 3), activation='relu'),\n","    layers.MaxPooling2D((2, 2)),\n","    layers.Flatten(),\n","    layers.Dense(512, activation='relu'),\n","    layers.Dense(len(train_dataset.class_names), activation='softmax')\n","])\n"]},{"cell_type":"markdown","metadata":{"id":"FFY_QilJQv_P"},"source":["4. Compilation of Neural Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n"]},{"cell_type":"markdown","metadata":{"id":"8Rt2cz2fQyIk"},"source":["5. Train the Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["history = model.fit(\n","    train_dataset,\n","    epochs=10,\n","    validation_data=validation_dataset\n",")\n","\n","# Evaluate the model\n","test_loss, test_acc = model.evaluate(test_dataset)\n","print(f'Test accuracy: {test_acc}')\n","\n","# Save the model\n","model.save('/content/drive/MyDrive/AI/Projeto_AI/image_classifier_model.h5')\n"]},{"cell_type":"markdown","metadata":{"id":"y3obr0riRHAK"},"source":["6. Show the Progress Graph"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# Plot training & validation accuracy values\n","plt.figure(figsize=(8, 8))\n","plt.subplot(2, 1, 1)\n","plt.plot(history.history['accuracy'], label='Training Accuracy')\n","plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.ylabel('Accuracy')\n","plt.ylim([0, 1.0])\n","plt.title('Training and Validation Accuracy')\n","\n","# Plot training & validation loss values\n","plt.subplot(2, 1, 2)\n","plt.plot(history.history['loss'], label='Training Loss')\n","plt.plot(history.history['val_loss'], label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.ylabel('Cross Entropy')\n","plt.ylim([0, 1.0])\n","plt.title('Training and Validation Loss')\n","plt.xlabel('epoch')\n","plt.show()\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/2212552/Projeto_IA_2023_2024/blob/Development_Carlos/main.ipynb","timestamp":1715359843494}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
